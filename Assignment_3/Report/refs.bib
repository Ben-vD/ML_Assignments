@Inbook{DT_Rokach2010,
    author={Rokach, Lior and Maimon, Oded},
    title={Classification Trees},
    bookTitle={Data Mining and Knowledge Discovery Handbook},
    year={2010},
    publisher={Springer US},
    address={Boston, MA},
    pages={149-174},
    abstract={Decision Trees are considered to be one of the most popular approaches for representing classifiers. Researchers from various disciplines such as statistics, machine learning, pattern recognition, and Data Mining have dealt with the issue of growing a decision tree from available data. This paper presents an updated survey of current methods for constructing decision tree classifiers in a top-down manner. The chapter suggests a unified algorithmic framework for presenting these algorithms and describes various splitting criteria and pruning methodologies.},
    isbn={978-0-387-09823-4},
    doi={10.1007/978-0-387-09823-4{\_}9},
    url={https://doi.org/10.1007/978-0-387-09823-4{\_}9}
}

@mastersthesis{GavinPotgieter,
  title        = {Mining continuous classes using evolutionary computing},
  author       = {Potgieter, Gavin},
  year         = {2006},
  school       = {University of Pretoria},
  url          = {http://hdl.handle.net/2263/26528}
}

@article{DT_Kotsiantis2013,
  author = {Kotsiantis, S. B},
  title = {Decision trees: a recent overview},
  year = {2013},
  journal = {Artificial Intelligence Review},
  volume = {39},
  pages = {261-283},
  doi = {10.1007/s10462-011-9272-4},
  url = {https://doi.org/10.1007/s10462-011-9272-4}
}

@article{DT_Blockeel2023,
    author={Blockeel, Hendrik and Devos, Laurens and Frénay, Benoît and Nanfack, Géraldin and Nijssen, Siegfried},
    title={Decision trees: From efficient prediction to responsible AI},
    year={2023},
    journal={Frontiers in Artificial Intelligence},
    isbn={978-0-387-09823-4},
    doi={10.3389/frai.2023.1124553},
    url={https://doi.org/10.3389/frai.2023.1124553}
}

@article{DT_Morgan1963ProblemsIT,
  title={Problems in the Analysis of Survey Data, and a Proposal},
  author={James N. Morgan and John A. Sonquist},
  journal={Journal of the American Statistical Association},
  year={1963},
  volume={58},
  pages={415-434},
  url={https://api.semanticscholar.org/CorpusID:1825515}
}

@article{DT_Kass1980,
  author = {Kass, G. V.},
  title = {An Exploratory Technique for Investigating Large Quantities of Categorical Data},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {29},
  number = {2},
  pages = {119-127},
  keywords = {aid, prediction, categorical data},
  doi = {https://doi.org/10.2307/2986296},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2986296},
  eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.2307/2986296},
  abstract = {Summary The technique set out in the paper, chaid, is an offshoot of aid (Automatic Interaction Detection) designed for a categorized dependent variable. Some important modifications which are relevant to standard aid include: built-in significance testing with the consequence of using the most significant predictor (rather than the most explanatory), multi-way splits (in contrast to binary) and a new type of predictor which is especially useful in handling missing information.},
  year = {1980}
}

@book{DT_breiman1984classification,
  title={Classification and Regression Trees},
  author={Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
  isbn={9780412048418},
  lccn={83019708},
  url={https://books.google.co.za/books?id=JwQx-WOmSyQC},
  year={1984},
  publisher={Taylor \& Francis}
}

@article{DT_quinlan1986,
  title={Induction of Decision Trees},
  author={J. Ross Quinlan},
  journal={Machine Learning},
  year={1986},
  volume={1},
  pages={81-106},
  url={https://api.semanticscholar.org/CorpusID:189902138}
}

@book{DT_quinlan1993,
  title={C4.5: Programs for Machine Learning},
  author={Quinlan, J.R.},
  isbn={9781558602380},
  lccn={92032653},
  series={Morgan Kaufmann series in machine learning},
  url={https://books.google.co.za/books?id=HExncpjbYroC},
  year={1993},
  publisher={Elsevier Science}
}

@book{Kelleher2015,
  author    = {John D. Kelleher and Brian MacNamee and Aoife D’Arcy},
  title     = {Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies},
  publisher = {The MIT Press},
  year      = {2015},
  address   = {Cambridge, Massachusetts},
  note      = {Print}
}

@book{Breiman1984,
  author    = {L. Breiman and J. H. Friedman and R. A. Olshen and C. J. Stone},
  title     = {Classification and Regression Trees},
  publisher = {Wadsworth International Group},
  year      = {1984}
}

@article{mienye2022survey,
  author = {I. D. Mienye and Y. Sun},
  title = {A Survey of Ensemble Learning: Concepts, Algorithms, Applications, and Prospects},
  journal = {IEEE Access},
  volume = {10},
  pages = {99129--99149},
  year = {2022},
  doi = {10.1109/ACCESS.2022.3207287},
  keywords = {Boosting;Classification algorithms;Prediction algorithms;Machine learning algorithms;Computational modeling;Bagging;Machine learning;Learning systems;Algorithms;classification;ensemble learning;fraud detection;machine learning;medical diagnosis}
}

@article{naderalvojoud2024improving,
  author = {Naderalvojoud, B. and Hernandez-Boussard, T.},
  title = {Improving machine learning with ensemble learning on observational healthcare data},
  journal = {AMIA Annu Symp Proc},
  year = {2024},
  month = {Jan 11},
  pages = {521--529},
  volume = {2023},
  pmid = {38222353},
  pmcid = {PMC10785929}
}

@article{zhou2021formulating,
  author = {J. Zhou and Z. Jiang and F. -L. Chung and S. Wang},
  title = {Formulating Ensemble Learning of SVMs Into a Single SVM Formulation by Negative Agreement Learning},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {51},
  number = {10},
  pages = {6015--6028},
  year = {2021},
  month = {Oct},
  doi = {10.1109/TSMC.2019.2958647},
  keywords = {Support vector machines;Training;Bagging;Optimization;Boosting;Kernel;Task analysis;Diversity;ensemble learning;negative agreement learning (NAL);support vector machines (SVMs);training error}
}

@article{mishra2022improving,
  author = {Mishra, S. and Shaw, K. and Mishra, D. and Patil, S. and Kotecha, K. and Kumar, S. and Bajaj, S.},
  title = {Improving the Accuracy of Ensemble Machine Learning Classification Models Using a Novel Bit-Fusion Algorithm for Healthcare AI Systems},
  journal = {Frontiers in Public Health},
  volume = {10},
  pages = {858282},
  year = {2022},
  doi = {10.3389/fpubh.2022.858282}
}

@article{sun2021classifier,
  author = {Sun, Y. and Li, Z. and Li, X. and Zhang, J.},
  title = {Classifier Selection and Ensemble Model for Multi-class Imbalance Learning in Education Grants Prediction},
  journal = {Applied Artificial Intelligence},
  volume = {35},
  number = {4},
  pages = {290--303},
  year = {2021},
  doi = {10.1080/08839514.2021.1877481}
}

@incollection{brown2011ensemble,
  author = {Brown, G.},
  title = {Ensemble Learning},
  booktitle = {Encyclopedia of Machine Learning},
  editor = {Sammut, C. and Webb, G. I.},
  publisher = {Springer},
  address = {Boston, MA},
  year = {2011},
  doi = {10.1007/978-0-387-30164-8_252}
}

@article{banfeld2007comparison,
  author = {Banfeld, R. and Bowyer, K. and Kegelmeyer, W. and Hall, L.},
  title = {A comparison of decision tree ensemble creation techniques},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {29},
  pages = {173--180},
  year = {2007}
}

@article{hernandezlobato2013large,
  author = {Hernandez-Lobato, D. and Martinez-Munoz, G. and Suarez, A.},
  title = {How large should ensembles of classifiers be?},
  journal = {Pattern Recognition},
  volume = {46},
  pages = {1323--1336},
  year = {2013}
}

@inproceedings{oshiro2012how,
  author = {Oshiro, T. and Perez, P. and Baranauskas, J.},
  title = {How many trees in a random forest?},
  booktitle = {International Workshop on Machine Learning and Data Mining in Pattern Recognition},
  pages = {154--168},
  publisher = {Springer},
  address = {Berlin},
  year = {2012}
}


@article{probst2018tune,
  author = {Probst, P. and Boulesteix, A.-L.},
  title = {To tune or not to tune the number of trees in a random forest?},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  pages = {1--18},
  year = {2018}
}

@article{freeman2015random,
  author = {Freeman, E. A. and Moisen, G. G. and Coulston, J. W. and Wilson, B. T.},
  title = {Random forests and stochastic gradient boosting for predicting tree canopy cover: comparing tuning processes and model performance},
  journal = {Canadian Journal of Forest Research},
  volume = {46},
  number = {3},
  pages = {323--339},
  year = {2015}
}

@article{huang2016parameter,
  author = {Huang, B. F. F. and Paul, C. B.},
  title = {The parameter sensitivity of random forests},
  journal = {BMC Bioinformatics},
  volume = {17},
  pages = {331},
  year = {2016}
}

@article{breiman2001random,
  author = {Breiman, L.},
  title = {Random forests},
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  year = {2001}
}

@inproceedings{ho1995random,
  author = {Ho, T. K.},
  title = {Random decision forests},
  booktitle = {Proceedings of the Third International Conference on Document Analysis and Recognition},
  volume = {1},
  pages = {278--282},
  year = {1995},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA}
}


@article{amit1997shape,
  author = {Amit, Y. and Geman, D.},
  title = {Shape quantization and recognition with randomized trees},
  journal = {Neural Computation},
  volume = {9},
  pages = {1545--1588},
  year = {1997}
}

@article{SUN2024121549,
  title = {An improved random forest based on the classification accuracy and correlation measurement of decision trees},
  journal = {Expert Systems with Applications},
  volume = {237},
  pages = {121549},
  year = {2024},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2023.121549},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417423020511},
  author = {Zhigang Sun and Guotao Wang and Pengfei Li and Hui Wang and Min Zhang and Xiaowen Liang},
  keywords = {Classification accuracy, Correlation measurement, Dot product, Random forest, CART},
  abstract = {Random forest is one of the most widely used machine learning algorithms. Decision trees used to construct the random forest may have low classification accuracies or high correlations, which affects the comprehensive performance of the random forest. Aiming at these problems, the authors proposed an improved random forest based on the classification accuracy and correlation measurement of decision trees in this paper. Its main idea includes two parts, one is retaining the classification and regression trees (CARTs) with better classification effects, the other is reducing the correlations between the CARTs. Specifically, in the classification effect evaluation part, each CART was applied to make predictions on three reserved data sets, then the average classification accuracies were achieved, respectively. Thus, all the CARTs were sorted in descending order according to their achieved average classification accuracies. In the correlation measurement part, the improved dot product method was proposed to calculate the cosine similarity, i.e., the correlation, between CARTs in the feature space. By using the achieved average classification accuracy as reference, the grid search method was used to find the inner product threshold. On this basis, the CARTs with low average classification accuracy among CART pairs whose inner product values are higher than the inner product threshold were marked as deletable. The achieved average classification accuracies and correlations of CARTs were comprehensively considered, those with high correlation and weak classification effect were deleted, and those with better quality were retained to construct the random forest. Multiple experiments show that, the proposed improved random forest achieved higher average classification accuracy than the five random forests used for comparison, and the lead was stable. The G-means and out-of-bag data (OBD) score obtained by the proposed improved random forest were also higher than the five random forests, and the lead was more obvious. In addition, the test results of three non-parametric tests show that, there were significant diversities between the proposed improved random forest and the other five random forests. This effectively proves the superiority and practicability of the proposed improved random forest.}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@book{hastie2001elements,
  title = {The Elements of Statistical Learning},
  author = {Hastie, T. and Tibshirani, R. and Friedman, J.},
  series = {Springer Series in Statistics},
  publisher = {Springer New York Inc.},
  address = {New York, NY, USA},
  year = {2001}
}
